---
title: "Lista 01 - Aprendizado de Máquinas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      fig.align='center',
                      cache=TRUE,
                      warning=FALSE,
                      message=FALSE)
library(tibble)
library(dplyr)
library(printr)
```

## Exercício 01

Faça uma pesquisa sobre aplicações práticas de aprendizado de máquinas e 
descreva aplicações de cada um dos problemas abaixo, incluindo a descrição
dos vetores de características, rótulos ou respostas quando apropriado para
cada um dos problemas:

a) Problema de classificação.

b) Problema de regressão.

c) Problema de Agrupamento.
## Solução Exercício 01

# Problema de classificação

- busca classificar dados
- não retorna uma caracteristica desconhecida
- kNN
---
O problema da classificação são situações em que o objetivo é classificar os dados em diferentes categorias. No caso do aprendizado de máquina, se utiliza dados rotulados afim de, para cada dado que futuramente seja inserido, o sistema seja capaz de diferenciar nas categorias treinadas.

Por exemplo, para determinar se uma maçã é boa ou está podre, podemos utilizar imagens de maçãs comestiveís e de maçãs aprodrecendo, e assim criar um sistema capaz de separar maçãs com base em fotos tiradas por uma câmera. Entretanto, caso utilizamos o mesmo sistema, sem nenhuma alteração, para separar pera, o sistema não sabera diferenciar quais peras são saudáveis e quais estão podres.

Uma técnica comum é utilizar o kNN..
# Problema de regressão

- Prever valores
- Pode retornar o que não conhece

# Problema de agrupamento

- Busca agrupar os dados por meios não supervisionados
- K-means
- Clustering
- Parametros/Estabilidade
- Density


## Exercício 02

Descreva com suas próprias palavras o que é a "maldição da dimensionalidade".

## Solução Exercício 02
- Dados organizados apenas em altas dimensões, ficam desordenados em baixa dimensão
- Quando aumenta a dimensão, aumenta o volume nos quais os dados podem ficar;
- As estratégias de organização ficam ineficientes;
- O poder de previsão aumenta até uma certa quantidade de dimensões, mas conforme for aumentando fica dificil prever.
- https://pt.wikipedia.org/wiki/Maldi%C3%A7%C3%A3o_da_dimensionalidade
----
A maldição da dimensionalidade é um problema existente em diversas areas que lidam com dados, dentre elas o aprendizado de máquina.

Para organizar dados, são necessários "dimensões" para separá-los, por exemplo: se queremos determinar se alguém está acima do peso ou não, meramente utilizar o quanto alguém pesa é insuficiente para classificar, e para corrigir isso podemos utilizar o tamanho da pessoa e a idade como parâmetro, aumentado a "dimensão" do nosso sistema, e assim conseguindo resultados mais precisos. Isso ocorre pois o "volume" nos quais os dados podem estar confinados aumenta, e assim ficam mais dispersos do que caso representassemos numa única dimensão.

O problema é que, conforme a dimensão aumenta, fica mais difícil estabelecer estratégias para organiza-los e, por mais que aumentar os parâmetros ajude a prever, utilizar muitas dimensões também começa a ter o efeito oposto, já que os dados começam a ficar dispersos demais afim de separá-los apropriadamente.


## Exercício 03

Implemente o método dos vizinhos mais próximos para um problema de
classificação. Sua função deve receber como argumento:

1) Um número $k$ representando o número de vizinhos para usar no kNN;

1) Um vetor $x = (x_1,x_2)$ com duas componentes;

2) Uma *dataframe* $D$ com colunas $x_1$, $x_2$ e $y$, em que $x_1$ e $x_2$ são 
valores numéricos representando duas componentes do vetor de características $x$
(i.e., $x = (x_1,x_2)$) e $y$ é um fator representando os rótulos dos pontos.
Abaixo tem-se um exemplo dessa *dataframe* com duas classes:
```{r}
D <- tibble( x_1 = rnorm(100,1,1),
             x_2 = rnorm(100,-1,2),
             y   = factor(sample(c("one","two","three"),100,replace = T)))
head(D)
```

A função deve ter a assinatura `function(k,x,D)` e deve 
retornar a classe mais provável associada ao ponto $x$.

***dica***: Você pode fazer o kNN usando uma sequencia de comandos encadeados
pelo operador pipe `%>%`. Por exemplo, teste a seguinte sequencia de 
comandos com a *dataframe* $D$ anterior:
```{r}
x = c(1,2)
k = 10
D2 <- D %>%
  mutate( dist = (x[1] - x_1)^2 + (x[2] - x_2)^2 ) %>%
  arrange( dist ) %>% head(k) %>% count(y)
```

## Solução Exercício 03
```{r}
#Dado um X, quantos vizinhos eu posso usar o kNN
#um numero (x1,x2)
#um dataframe

library(magrittr)
#%>% = encadear funcoes
calculaTudo <- function(k,x,D){
  D2 <- D %>%
    mutate( dist = (x[1] - x_1)^2 + (x[2] - x_2)^2 ) %>%
    arrange( dist ) %>% head(k) %>% count(y)
  p=D #temp
  return(D2)
}

x_1 = rnorm(100,1,1)
x_2 = rnorm(100,-1,2)
x <- c(x_1, x_2)

Data <- tibble( x_1 = rnorm(100,1,1),
             x_2 = rnorm(100,-1,2),
             y   = factor(sample(c("one","two","three"),100,replace = T)))
head(D)
print(calculaTudo(10,x,Data))
```

## Exercício 04

Usando o banco de dados `iris` e sua implementação do kNN do exercício anterior,
calcule quantos pontos são classificados corretamente
de acordo com o rótulo `Species` usando
as colunas `Petal.length` e `Sepal.length` com $k = 10$ e com $k = 1$. 

***dica 1***: Você pode carregar o banco Iris no R da seguinte forma:
```{r}
library(tidyverse)
data("iris") # Carrega o banco no ambiente global
iris <- as_tibble(iris) %>%  # Converte para a dataframe tibble
  select(Petal.Length,Sepal.Length,Species) %>% # Seleciona colunas da dataframe
  rename( x_1 = Petal.Length, x_2 = Sepal.Length, y = Species) # Renomeia as colunas

head(iris)
```

***dica 2***: As funções `map` da biblioteca `purrr` do pacote *tidyverse* são muito
úteis! Por exemplo, a função `pmap_lgl` aplica uma função à argumentos fornecidos
por uma lista e retorna os resultados da função como um vetor de booleanos 
(neste caso, a função deve retornar valores booleanos). Rode o exemplo 
abaixo:
```{r results = "hide"}
l_iris <- as.list(iris) # converte a dataframe em uma lista
                        # que possui elementos com nomes x_1, x_2 e y

# Aplica a função usando os valores dados pelos valores na lista,
#  concatena os resultados da função em um vetor booleano.
v_bool <- pmap_lgl(l_iris, function(x_1,x_2,y){
          print(str_c(x_1,", ",x_2,", ",y))
          return( y == "setosa" )
})
```
A função `sum` pode também ser útil. Lembre-se de usar o comando `?` para ajuda.

## Solução Exercício 04
```{r}
## copiando do enunciado
library(tidyverse)
library(magrittr)

data("iris") # Carrega o banco no ambiente global
iris <- as_tibble(iris) %>%  # Converte para a dataframe tibble
  select(Petal.Length,Sepal.Length,Species) %>% # Seleciona colunas da dataframe
  rename( x_1 = Petal.Length, x_2 = Sepal.Length, y = Species) # Renomeia as colunas

head(iris)

## copiando do exercicio anterior

calculaTudo <- function(k,x,D){
  D2 <- D %>%
    mutate( dist = (x[1] - x_1)^2 + (x[2] - x_2)^2 ) %>%
    arrange( dist ) %>% head(k) %>% count(y)
  p=D #temp
  return(D2)
}


x_1 = rnorm(100,1,1)
x_2 = rnorm(100,-1,2)
x <- c(x_1, x_2)
##

print(calculaTudo(10,x,iris))
print(calculaTudo(1,x,iris))
```

## Exercício 5 (opcional)

Em aula vimos como calcular a função de regressão $f:\mathcal{X}\rightarrow
\mathcal{Y}$ ótima que minimiza o risco esperado:
\[
  \mathcal{R}(f) = \mathbb{E}_{XY}[ \ell(Y,f(X)) ]
\]
quando a função de perda é dada por $\ell_2(y,y') := (y-y')^2$. Essa função de 
perda é geralmente usada pela simplicidade de soluções. Mas existem outras
funções de perda, como a função de perda do erro absoluto, que é dada por:
$\ell_1(y,y') := |y-y'|$. Mostre que a função $f$ ótima, que minimiza o 
risco esperado com essa função de perda, é dada por 
$f(x):= \mbox{Mediana}(Y| X= x)$. Suponha que a distribuição de $Y$ é
contínua.

***dica 1***: A mediana de uma variável aleatória contínua tomando valor
em $\mathbb{R}$ é definida como sendo o valor real $m$ tal que $P( Y > m) = P(Y < m) = 1/2$.

***dica 2***: A derivada de $\ell_1(y,y')$ em relação a $y'$ quando $y' \neq y$
existe e é limitada. Nestes casos, sabe-se que
\[
  \frac{\partial}{\partial z}\mathbb{E}[ \ell_1(Y,z) | X = x] = 
  \mathbb{E}\Big[ \frac{\partial}{\partial z} \ell_1(Y,z) \Big| X = x\Big].
\]

## Solução Exercício 05
```{r}

```


## Exercício 6 (opcional)

Considere que $m$ pontos são espalhados uniformemente em uma hiperesfera de raio unitário e dimensão $d$.
Mostre que a mediana da distância do ponto mais próximo à origem é dada por: $(1-0.5^{1/m})^{1/d}$.
## Solução Exercício 06
```{r}

```
